# Reproduction Guide
We give a full walkthrough of all results in the report. Discretion is advised as repository deals with jailbreak prompts, which often contain malicious, unsafe, or inappropriate text.

## Setup
Requirements can be installed with `pip install -r requirements.txt`. The project is also intended to be compatible with `uv`.

All code currently assumes availability of 1 GPU, in particular a Radeon W7900 with 48GB of VRAM. Batch sizes should be adjusted accordingly.

For downloading the trained SAEs, we use Git LFS.

## Dataset
`python prompt_dataset.py` pulls and generates the prompt dataset. 

Figure 1 is generated by `python evaluate_classifier_on_dataset.py`.

## Autoencoder Training
Training is handled by `python train_sae.py`. This creates a weights-and-biases run that can be viewed live on the website, if `use_wandb` is set. The account name and project name are read from `.env`, which follows the template from `.env.example`.

The training process takes around 2 hours per layer.

The loss curves et cetera can be obtained via `python plot_wandb_loss.py`, which automatically scans the wandb project specified in `.env` for the last runs. Note that this assumes that all 12 layers were trained on. This produces Figure 4.

A finished training run on all layers can be pulled with Git LFS.

## Assessment of SAE Quality
The logistic regression is done with `python test_logistic_regression.py`. This also precalculates the important features to be used in the HTML visualization later.

Aligned UMAP experiments are done with `python cluster.py`. This produces Figure 2 and 5. 

## Discovery of Semantic SAE Features + An SAE-Assisted Discovery of a False Positive Attack
The HTML visualization is created with `python test_visual.py`. This script can take up to 20 minutes to run. The resulting page can be opened in any web browser, we tested on Firefox. The leetspeak and string appending activation change experiments are done with `python test_activation_after transformation.py` after setting `TRANSFORM_FN` to either of `{append_omega, append_omega_infared, append_infared, leet_speak}`. This produces the mean activation change values and Figure 3.

## Finding Mislabeled Samples
Caution, reproducing this section will show malicious text: Table 5 was created by manually assessing (1) manually picking 25 points that were clustered with many others with a different label, in `figures/layer_10/aligned_umap_embedding_layer_10.html`, and (2) the 25 samples produced by `uncertain_samples.ipynb`.

## Hardening the Classifier against Adversarial Inputs 
The ensemble of the leetspeak SAE feature and the original classifier is run with `python combined_classifier.py`. 


